# 앙상블 분석 / 앙상블 학습 기법
- 데이터 마이닝에서는 여러 개의 모형을 생성 및 조합하여 예측력이 높은 모형을 만드는 것을 의미한다.
- 모형의 예측력을 높이괒 여러 번의 데이터 분할을 통해 구축된 다수의 모형을 결합하여 새로운 모형을 만드는 방법
- 대표적인 앙상블 분석 방법 : 배깅, 부스팅, 랜덤 포레스트, 스태킹
- 앙상블 분석 장점 :
  - 각각의 예측모형에서 독립적으로 산출된 결과를 종합하여 예측의 정확도를 향상시킬 수 있음
  - 종속 변수의 형태에 따라 회귀분석과 분류 분석에 모두 적용할 수 있다.
- 앙상블 분석 특징 :
  - 결과가 수치형 데이터인 경우 : 값들의 평균을 통해 최종 결과를 예측
  - 결과가 범주형 데이터인 경우 : 다수결 방식으로 최종 결과를 예측
 
## 1. 배깅 (bagging, Bootstrap Aggregating)
- 배깅은 Bootstrap Aggregating의 줄임말.
- 여러 개의 붓스트랩을 생성하여 모델을 학습한 후에 voting으로 결합하는 알고리즘
  - 붓스트랩 : 원본 데이터와 같은 크기의 표본을 랜덤복원추출한 샘플 데이터 (모델구축용 train 데이터)
  - voting : 여러 개의 분류기에 의한 결과를 놓고, 다수결에 의해 최종 결과값을 선정하는 작업
- 배깅 장점 : 모집단의 특성이 잘 반영되며, 분산이 작고, 좋은 예측력을 보여줌

## 2. 부스팅 (Boosting)
- 이전 모델을 구축한 뒤, 다음 모델을 구축할 때, 이전 분류기에 의해 잘못 분류된 데이터에 더 큰 가중치를 주어 붓스트랩을 구성함.
  - 따라서 약한 모델들을 결합해 나감으로써 점차적으로 강한 분류기를 만들어 나가는 과정.
- 배깅과의 공통,차이점
  - 부스팅과 배깅은 여러 개의 모형을 구축한다는 점에서 같음
  - 하지만 배깅은 각 분류기(모델)가 독립적임 / 부스팅은 반면 각 모델이 독립적이지 않음.
- 대표적인 부스팅 방법 : AdaBoosting, Gradient Boost, XGBoost, Light GBM
- 부스팅 장점 : 훈련오차를 빠르게 줄일 수 있음. 배깅보다 예측성능이 뛰어남

## 3. 랜덤 포레스트 (Random Forest)
- 서로 상관성이 없는 나무들로 이루어진 숲
- 배깅에 의사결정트리를 추가한 개념. 배깅에 더 많은 무작위성을 주는 분석 기법이다.
- 많은 무작위성으로 생성된 서로 다른 여러 개의 트리로 구성되어, 여러 개의 약한 트리들의 선형 결합으로 최종 결과를 얻는 모델
- 분류라면 다수결로 최종결과 구함 / 회귀라면 평균 또는 중앙값을 구함
- 배깅과의 공통, 차이점
  - 배깅 : 각 붓스트랩을 활용하여 트리를 구성할 때, 트리의 모든 마디가 불순도가 제일 작아지는 최적의 분할을 실시 (마디에서 최적분할)
  - 랜덤 포레스트 : 표본 추출 과정이 한번 더 반복되어, 추출된 표본을 대상으로 최적의 분할을 실시 (추출된 표본에서 최적분할)
- 랜덤 포레스트 장점 :
  - 분산을 감소시킴
  - 모든 분류기들이 높은 비상관성을 갖고 있기에 일반화의 성능 향상 가능
  - 이상값에 민감하지 않음

## 4. 스태킹 (Stacking)
- 위의 3개 (배깅, 부스팅, 랜덤 포레스트)는 각 분류기 간의 결과를 활용해 보팅(다수결) 방식에 의해 최종 결과를 선정함
- but, 스태킹은 보팅방식이 아닌, 여러 분류기(base model) 간의 결과를 다시 train data로 사용하여 최종 모형인 meta model을 구축하는 방법
- 여러 개의 분류기(base model)는 서로 다른 알고리즘으로 작성될 수 있음
- 스태킹 장점 : 서로 다른 모델은 각자의 약점을 보완, 높은 예측력 가짐
- 스태킹 단점 : 높은 복잡도와 오랜 학습시간, 결과의 해석이 어려움
